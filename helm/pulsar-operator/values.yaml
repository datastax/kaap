---
## Whether to create the Operator RBAC.
rbac:
  create: true

## Service account name and whether to create it.
serviceAccount:
  create: true
  name: pulsar-operator

## Whether to create the CRD.
crd:
  create: true


operator:
  image: datastax/pulsar-operator:latest
  imagePullPolicy: IfNotPresent
  livenessProbe:
    failureThreshold: 3
    periodSeconds: 30
    timeoutSeconds: 10
    successThreshold: 1
    initialDelaySeconds: 0

  readinessProbe:
    failureThreshold: 3
    timeoutSeconds: 10
    periodSeconds: 30
    successThreshold: 1
    initialDelaySeconds: 0

# Deploy Grafana dashboard for Pulsar.
# These dashboards will be discovered by the kube-prometheus-stack
# if it is running the same namespace as the stack

grafanaDashboards:
  enabled: false
#  namespaceOverride: monitoring

# Deploy the kube-prometheus-stack which includes:
# * Prometheus (using Prometheus Operator)
# * Alertmanager (using Prometheus Operator)
# * Grafana
# * Prometheus node exporter
#
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
#
# The default values will enable Prometheus and
# Grafana, but not AlertManager.
# Scraping configs for Pulsar pods are included. By default,
# pods in the namespace of the deployment will be scraped.
#
# The stack can add standard Kubernetes alert rules, but those
# are disabled by default.
#
# A set of Pulsar-specific rules are included below.
# The threshold for some of the rules are low and should
# be adjusted to match the performance/use of the Pulsar
# deployment. To be notified when the rules are triggered,
# you need to enable and configure Alertmanager. It supports
# many targets including email, Slack, OpsGenie, PagerDuty.

kube-prometheus-stack:
  enabled: false
  # namespaceOverride: monitoring
  prometheus-node-exporter:
  # namespaceOverride: monitoring
  defaultRules:
    create: false
    rules:
      alertmanager: true
      etcd: true
      general: true
      k8s: true
      kubeApiserver: true
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubeScheduler: true
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      node: true
      prometheusOperator: true
      prometheus: true
  additionalPrometheusRulesMap:
    general-rules:
      groups:
        - name: acks
          rules:
            - alert: unacked-message-high
              expr: pulsar_subscription_unacked_messages > 100
              for: 1m
              labels:
                severity: critical
              annotations:
                identifier: '{{ $labels.topic }}:{{ $labels.subscription }}'
                description: 'Unacked messages on subscription high'
            - alert: subscription-blocked-on-unacked-messages
              expr: pulsar_subscription_blocked_on_unacked_messages > 1
              for: 1m
              labels:
                severity: critical
              annotations:
                identifier: '{{ $labels.topic }}:{{ $labels.subscription }}'
                description: Subscription is blocked on unacked messages
        - name: components
          rules:
            - alert: zookeeper-write-latency-high
              expr: zookeeper_server_requests_latency_ms > 500
              for: 1m
              labels:
                severity: warning
              annotations:
                identifier: '{{ $labels.kubernetes_pod_name }}'
                description: 'Zookeeper write latency is high'
            - alert: bookkeeper-add-latency-high
              expr: bookkeeper_server_ADD_ENTRY_REQUEST > 1500
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Add latency to BookKeeper is high
                identifier: '{{ $labels.kubernetes_pod_name }}'
            - alert: bookkeeper-read-latency-high
              expr: bookkeeper_server_READ_ENTRY_REQUEST > 1000
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Read latency from BookKeeper is high
                identifier: '{{ $labels.kubernetes_pod_name }}'
            - alert: bookkeeper-bookie-readonly
              expr: bookie_SERVER_STATUS == 0
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Bookie is read-only
                identifier: '{{ $labels.kubernetes_pod_name }}'
    cluster-normal:
      groups:
        - name: exp-counts-rates
          rules:
            - alert: producers-high
              expr: pulsar_producers_count > 500
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Producer count is high
                identifier: '{{ $labels.topic }}'
            - alert: consumers-high
              expr: pulsar_consumers_count > 500
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Consumer count is high
                identifier: '{{ $labels.topic }}'
            - alert: in-rate-high
              expr: pulsar_rate_in > 5000
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Incoming message rate is high on broker
                identifier: '{{ $labels.topic }}'
            - alert: out-rate-high
              expr: pulsar_rate_out > 5000
              for: 1m
              labels:
                severity: warning
              annotations:
                description: Outgoing message rate is high on broker
                identifier: '{{ $labels.topic }}'
    volumes:
      groups:
        - name: volumes-filling
          rules:
            - alert: KubePersistentVolumeFillingUp
              expr: kubelet_volume_stats_available_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"} / kubelet_volume_stats_capacity_bytes{job="kubelet",metrics_path="/metrics",namespace=~".*"} < 0.10
              for: 1m
              labels:
                severity: critical
              annotations:
                message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.

  alertmanager:
    enabled: false

  grafana:
    enabled: true
    # namespaceOverride: "monitoring"
    testFramework:
      enabled: false
    defaultDashboardsEnabled: true
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: true
            editable: false
            options:
              path: /var/lib/grafana/dashboards/default
    dashboards:
      default:
        zookeeper:
          gnetId: 10465
          revision: 4
          datasource: Prometheus
    # Configure to set a default admin password for Grafana
    adminPassword:
    service:
      type: LoadBalancer
      port: 3000
    ingress:
      enabled: false
      hosts:
        - grafana.example.com
      path: /
    grafana.ini:
      server:
        root_url: 'http://localhost:3000'
      security:
        allow_embedding: true
        cookie_samesite: 'lax'

  kubeApiServer:
    enabled: true
  kubelet:
    enabled: true
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: true
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: true
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false
  kubeStateMetrics:
    enabled: true
  nodeExporter:
    enabled: true

  prometheusOperator:
    enabled: true

  prometheus:
    enabled: true
    ingress:
      enabled: false
    prometheusSpec:
      retention: 10d
      # storageSpec:
      #   volumeClaimTemplate:
      #     spec:
      #       storageClassName: default
      #       resources:
      #         requests:
      #           storage: 50Gi
      additionalScrapeConfigs:
        - job_name: 'pulsar-pods'
          honor_labels: true
          kubernetes_sd_configs:
            - role: pod
              # namespaces:
              #   names:
              #   - pulsar
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_pod_label_component]
              action: replace
              target_label: job
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name
